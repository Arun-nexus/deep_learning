{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arun-nexus/deep_learning/blob/main/pytorch_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQhFj96yJf4U"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "EEz-tD9jCgfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp \"/content/drive/MyDrive/kaggle.json\" ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "CK2Oa7ImJx3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d pawankumargunjan/imdb-review"
      ],
      "metadata": {
        "id": "ycYEI1bWKX9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"imdb-review.zip\",\"r\") as zip:\n",
        "  zip.extractall(\"imdb\")"
      ],
      "metadata": {
        "id": "hGXDlO-yLSpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "a4745mPeLuWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "test_dir=\"/content/imdb/aclImdb/test\"\n",
        "train_dir=\"/content/imdb/aclImdb/train\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for sentiment in [\"pos\", \"neg\"]:\n",
        "    sentiment_dir = os.path.join(test_dir, sentiment)\n",
        "    for filename in os.listdir(sentiment_dir):\n",
        "        filepath = os.path.join(sentiment_dir, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                review = f.read()\n",
        "                data.append({\"reviews\": review, \"label\": sentiment})\n",
        "\n",
        "for sentiment in [\"pos\", \"neg\"]:\n",
        "    sentiment_dir = os.path.join(train_dir, sentiment)\n",
        "    for filename in os.listdir(sentiment_dir):\n",
        "        filepath = os.path.join(sentiment_dir, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                review = f.read()\n",
        "                data.append({\"reviews\": review, \"label\": sentiment})\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "rTZXL58ILfe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[:10000]\n",
        "df"
      ],
      "metadata": {
        "id": "VwSlttpnMvpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "blfqoOFvMPKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lematizer=WordNetLemmatizer()\n",
        "def lemmatizer(review):\n",
        "  lemmatize_row=[lematizer.lemmatize(word) for word in review.split()]\n",
        "  return (lemmatize_row)\n",
        "df['reviews']=df[\"reviews\"].apply(lemmatizer)"
      ],
      "metadata": {
        "id": "HK7poj7fMmfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_list = [\" \".join(review_list) for review_list in df[\"reviews\"].to_list()]"
      ],
      "metadata": {
        "id": "vi5l4oR2PHcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_list[:10]"
      ],
      "metadata": {
        "id": "9Of66kgQYIUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "n_3ytoMePhpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize = TfidfVectorizer()\n",
        "review_vectors = vectorize.fit_transform(review_list)"
      ],
      "metadata": {
        "id": "IjwjzWNTRGO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_vector=review_vectors"
      ],
      "metadata": {
        "id": "VpDTiY4zRvdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9b6dd64"
      },
      "source": [
        "import torch\n",
        "review_dense = review_vectors.todense()\n",
        "tensor = torch.tensor(review_dense)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor=tensor[:1000,:50]\n",
        "tensor.shape"
      ],
      "metadata": {
        "id": "MEs3KkJ_alND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=df[\"label\"]\n",
        "Y=y.map({\"pos\":1,\"neg\":0})"
      ],
      "metadata": {
        "id": "IJPaOTrFaolF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "a_i4ZKdjbwfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sentiment(nn.Module):\n",
        "  def __init__(self,input_size, num_classes=1):\n",
        "     super(sentiment,self).__init__()\n",
        "     self.lstm = nn.LSTM(input_size, 128, 2, batch_first=True)\n",
        "     self.linear = nn.Linear(128, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x.unsqueeze(1)\n",
        "\n",
        "    _, (hidden, _) = self.lstm(x)\n",
        "    output = self.linear(hidden[-1])\n",
        "    return output"
      ],
      "metadata": {
        "id": "FxYwAE0fa4m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class earlystopping():\n",
        "  def __init__(self,min_delta,patience=2):\n",
        "    self.min_delta=min_delta\n",
        "    self.patience=patience\n",
        "    self.counter=0\n",
        "    self.best_loss=float(\"inf\")\n",
        "    self.early_stop=False\n",
        "\n",
        "  def __call__(self,val_loss):\n",
        "    if val_loss < self.best_loss-self.min_delta:\n",
        "      self.best_loss=val_loss\n",
        "      self.counter=0\n",
        "    else:\n",
        "       self.counter+=1\n",
        "       if self.counter>=self.patience:\n",
        "        self.early_stop=True"
      ],
      "metadata": {
        "id": "xFdCpRQzbNRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0oMc2JwoFyUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "M6ovlGqdGArL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=sentiment(input_size=tensor.shape[1],num_classes=1).to(device)"
      ],
      "metadata": {
        "id": "y9MGROSbD8LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor=tensor.to(torch.float32)"
      ],
      "metadata": {
        "id": "_ScFzwK9WjRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.amp import GradScaler,autocast"
      ],
      "metadata": {
        "id": "RAtFfU-WGwv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss=nn.BCEWithLogitsLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "scheduler=ReduceLROnPlateau(optimizer,mode=\"min\",patience=2,factor=0.5)"
      ],
      "metadata": {
        "id": "uNgotqRvGOiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader,dataset"
      ],
      "metadata": {
        "id": "U3_NZJ6LJXC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "if not isinstance(Y, torch.Tensor):\n",
        "    Y = torch.tensor(Y.values)\n",
        "dataset = ReviewDataset(tensor, Y)\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "7qLoSIzvPvku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=50\n",
        "val_loss=[]\n",
        "val_acc=[]\n",
        "training_loss=[]\n",
        "train_acc=[]\n",
        "scaler=torch.amp.GradScaler(\"cuda\")\n",
        "stopper=earlystopping(min_delta=0.0004,patience=4)\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss=0\n",
        "  validation_loss=0\n",
        "  correct=0\n",
        "  for x,y in data_loader:\n",
        "    y=y.float().unsqueeze(1)\n",
        "    x,y=x.to(device),y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    with autocast(device_type=\"cuda\"):\n",
        "      output=model(x)\n",
        "      loss=model_loss(output,y)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    running_loss+=loss.item()\n",
        "    preds = torch.sigmoid(output) > 0.5\n",
        "    correct+=(preds==y).sum().item()\n",
        "    validation_loss+=y.size(0)\n",
        "\n",
        "  train_loss=running_loss/len(data_loader)\n",
        "  acc=100*correct/validation_loss\n",
        "  training_loss.append(train_loss)\n",
        "  train_acc.append(acc)\n",
        "\n",
        "  print(f\"epoch no.{epoch+1} training_acc : {acc:.2f} training loss : {train_loss:.4f}\")\n",
        "\n",
        "  stopper(train_loss)\n",
        "\n",
        "  if stopper.early_stop:\n",
        "    print(\"early stopping\")\n",
        "    break"
      ],
      "metadata": {
        "id": "7fuKh9BqIF8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9V2MQXMV6KD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}